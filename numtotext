from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, lower, trim, expr, explode, array_except, broadcast

# Initialize Spark session with Hive support
spark = SparkSession.builder \
    .appName("OptimizedFuzzyMatching") \
    .enableHiveSupport() \
    .getOrCreate()

# Business suffix and stopwords
BSNS_SUFFIX_TAG_LIST = ["limited", "liability", "incorporated", "corporation", "association",
                        "solutions", "group", "partnership", "company", "pledge", "plc", "llp", "pvt", "co"]
ENGLISH_STOP_WORDS = ["the", "and", "of", "in", "to", "for", "on", "at", "by", "with"]

# Convert to broadcast sets for performance
bsns_bcast = broadcast(spark.createDataFrame([(w,) for w in BSNS_SUFFIX_TAG_LIST], ["word"]))
stopwords_bcast = broadcast(spark.createDataFrame([(w,) for w in ENGLISH_STOP_WORDS], ["word"]))

# Load data from Hive
df = spark.read.table("dpm_curated.eph.df_dummy_kevin")

# Tokenization and preprocessing
df_cleaned = df.withColumn("name_clean", lower(trim(col("name")))) \
               .withColumn("name_clean", expr(r"regexp_replace(name_clean, '[^a-z0-9 ]', '')")) \
               .withColumn("firstname", split(col("name_clean"), " ")[0]) \
               .withColumn("token", col("firstname") + col("postal_code"))

# Token Filtering (Remove business suffixes and stopwords)
df_tokenized = df_cleaned.withColumn("key_list", split(col("name_clean"), " ")) \
                         .withColumn("filtered_keys", array_except(col("key_list"), bsns_bcast["word"])) \
                         .withColumn("filtered_keys", array_except(col("filtered_keys"), stopwords_bcast["word"])) \
                         .withColumn("token", explode(col("filtered_keys")) + col("postal_code"))

# Optimize join using partitioning strategy
df_partitioned = df_tokenized.repartition("token")

# Self-Join on token (keeping all columns)
df_joined = df_partitioned.alias("a").join(
    df_partitioned.alias("b"),
    (col("a.token") == col("b.token")) & (col("a.id") != col("b.id")),
    "inner"
).select([col("a." + c) for c in df.columns] + [col("b.id").alias("match_id")])

# Write optimized output back to Hive
df_joined.write.mode("overwrite").partitionBy("postal_code").format("parquet") \
    .saveAsTable("dpm_curated.eph.df_final")
