from pyspark import StorageLevel

def safe_cache(df, name="DataFrame"):
    print(f"Attempting to cache {name} in memory...")
    df.cache()
    df.count()  # Trigger evaluation

    storage_info = spark._jsparkSession.sharedState().cacheManager().lookupCachedData(df._jdf)

    if storage_info.isEmpty():
        print(f"{name} NOT fully cached. Falling back to MEMORY_AND_DISK.")
        df.unpersist()
        df.persist(StorageLevel.MEMORY_AND_DISK)
        df.count()  # Trigger persist
    else:
        print(f"{name} successfully cached in memory.")

    return df
