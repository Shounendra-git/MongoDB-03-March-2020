
from pyspark.sql import SparkSession
from pyspark.sql.functions import lower, trim, regexp_replace, concat_ws, col

# Initialize Spark session
spark = SparkSession.builder.appName("TokenIDSelfJoin").getOrCreate()

# Load the dataset (update path as needed)
df = spark.read.option("header", True).csv("/mnt/data/Daata.csv")

# Tokenize name and postal_code
df_tokens = df.withColumn("token_name", lower(trim(regexp_replace(col("name"), "[^a-zA-Z0-9]", "")))) \
              .withColumn("token_postal", lower(trim(regexp_replace(col("postal_code"), "[^a-zA-Z0-9]", "")))) \
              .withColumn("token_key", concat_ws("_", col("token_name"), col("token_postal"))) \
              .select("id", "token_key")

# Join token back to original data
df_with_token = df.join(df_tokens, on="id", how="inner")

# Optional: Repartition by token_key to optimize join
df_with_token = df_with_token.repartition(2000, "token_key")

# Self-join on token_key, excluding identical IDs
df_result = df_with_token.alias("a").join(
    df_with_token.alias("b"),
    (col("a.token_key") == col("b.token_key")) & (col("a.id") != col("b.id")),
    "inner"
).select(
    col("a.id").alias("original_id"),
    col("a.name").alias("original_name"),
    col("a.postal_code").alias("original_postal_code"),
    col("b.id").alias("matched_id"),
    col("b.name").alias("matched_name"),
    col("b.postal_code").alias("matched_postal_code"),
    col("a.token_key")
)

# Show or save results
df_result.show(100, truncate=False)
# df_result.write.option("header", True).csv("/output/path/self_join_result")
