from pyspark.sql import SparkSession
from pyspark.sql.functions import col, explode, split, trim, lower, concat_ws, array, monotonically_increasing_id

spark = SparkSession.builder.getOrCreate()

# Load CSV
df = spark.read.option("header", True).csv("/mnt/data/Daata.csv")

# Add unique ID if not already present
df = df.withColumn("uid", monotonically_increasing_id())

# Clean and tokenize name + postal_code
df_tokenized = df.withColumn("name_clean", lower(trim(col("name")))) \
    .withColumn("postal_clean", lower(trim(col("postal_code")))) \
    .withColumn("name_tokens", split(col("name_clean"), "\\s+")) \
    .withColumn("postal_tokens", split(col("postal_clean"), "\\s+")) \
    .withColumn("tokens", array(*[col("name_tokens")[i] for i in range(2)] + [col("postal_tokens")[0]]))  # Simple merge of tokens

# Explode tokens to get 1:N format
exploded_df = df_tokenized.select("uid", "id", "name", "postal_code", explode("tokens").alias("token"))

# Self-join on token
joined = exploded_df.alias("left").join(
    exploded_df.alias("right"),
    (col("left.token") == col("right.token")) &
    (col("left.uid") != col("right.uid"))
)

# Optional: Add original data
result = joined.select(
    col("left.uid").alias("uid_l"), col("left.id").alias("id_l"),
    col("left.name").alias("name_l"), col("left.postal_code").alias("postal_code_l"),
    col("right.uid").alias("uid_r"), col("right.id").alias("id_r"),
    col("right.name").alias("name_r"), col("right.postal_code").alias("postal_code_r"),
    col("left.token")
)

result.show(truncate=False)
